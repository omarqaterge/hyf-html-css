<head>
	<title>Artificial intelligence</title>
	<link rel="stylesheet" type="text/css" href="style.css">
</head>
</head>

<body>
	<div id="sidebar">
		<h1>Artificial Intelligence <img src="https://static.wixstatic.com/media/132e12_725084cfa6394498a2101325befb7333~mv2.png/v1/fill/w_426,h_426,al_c,q_80,usm_0.66_1.00_0.01/132e12_725084cfa6394498a2101325befb7333~mv2.webp" alt="brain" style="float:left;width:42px;margin-right:4px;"></h1>

		<p>Humanity's New Hope ?</p>
		<ul id="nav">
			<li><a class="effect-shine" href="#what-is-ai"><b>»</b> WHAT IS A.I ?</a></li>
			<li><a class="effect-shine" href="#Symphony-of-AI"><b>»</b>The Symphony of AI</a></li>
			<li><a class="effect-shine" href="#machine-learning"><b>»</b>Machine Learning</a></li>
			<li><a class="effect-shine" href="#Deep-learning"><b>»</b>Deep learning</a></li>
			<li><a class="effect-shine" href="#nlp"><b>»</b>Natural Language Processing</a></li>
			<li><a class="effect-shine" href="#Deep-Learning-Breakthroughs"><b>»</b>Latest Deep Learning Breakthroughs</a>
			</li>
		</ul>
	</div>
	<div class="content">
		<h2 id="what-is-ai">WHAT IS AI ? </h2>

		<img src="https://internetofbusiness.com/wp-content/uploads/2018/09/artificialintelligence-1200x673.jpg" style="float:right;width:600px;clear:none;" alt="ai Illustration">

		<p id="start"><img src="https://ya-webdesign.com/images250_/blue-letter-f-png-6.png" alt="F" width="100px"> rom SIRI to self-driving cars, artificial intelligence (AI) is progressing rapidly. While science fiction often portrays AI as robots with human-like characteristics,
			AI can encompass anything from Google’s search algorithms to IBM’s Watson to autonomous weapons.<br><br> Artificial intelligence today is properly known as <a href="https://en.wikipedia.org/wiki/Weak_AI"> narrow AI (or weak AI)</a>, in that it is designed
			to perform a narrow task (e.g. only facial recognition or only internet searches or only driving a car). However, the long-term goal of many researchers is to create <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">general AI (AGI or strong AI)</a>.
			While narrow AI may outperform humans at whatever its specific task is, like playing chess or solving equations, AGI would outperform humans at nearly every cognitive task. </p>

		<h2 id="Symphony-of-AI">The Symphony of AI</h2>
		<img src="https://i2.wp.com/trendintech.com/wp-content/uploads/2016/07/Music-Brain.jpg?fit=1649%2C1167" alt="brain symphony" width="600px">
		<p> AGI « <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">Artificial general intelligence</a> », is the Grand Finale at the end of a symphony.<br> But before we strike that last glorious chord, each individual instrument must be
			played with great expertise. At Snips we believe the cornerstone instruments of AI include: Machine Learning, Deep Learning, Natural Language Understanding, Context Awareness, and Data Privacy. Play on! </p>

		<h2 id="machine-learning">Machine Learning</h2>
		<img src="https://www.kdnuggets.com/wp-content/uploads/machine-learning-algorithms-blackboard.jpg" alt="machine learning" width="600px">
		<p> <a href="https://en.wikipedia.org/wiki/Machine_learning"> Machine learning </a> and AI are not the same. Machine learning is an instrument in the AI symphony — a component of AI. So what is Machine Learning — or ML — exactly? It’s the ability for an
			algorithm to learn from prior data in order to produce a behavior. ML is teaching machines to make decisions in situations they have never seen.<br><br> The most mainstream approach to ML is showing the algorithm a data set of situations and telling
			it what the right decision is — training a model. This is supervised Machine Learning. Once the model has been trained, we can feed new, more foreign data through the algorithm — and hopefully, the machine makes intelligent decisions in these new,
			foreign situations. </p>

		<h2 id="Deep-learning">Deep learning</h2>

		<img src="https://cdn-images-1.medium.com/max/2000/1*5c661k4fUEL010pjFQq4CA.png" alt="deep learning" width="600px">

		<p> <a href="https://en.wikipedia.org/wiki/Deep_learning">Deep learning</a> is a branch of machine learning where artificial neural networks — algorithms inspired by the way neurons work in the brain — find patterns in raw data by combining multiple layers
			of artificial neurons. As the layers increase, so does the neural network’s ability to learn increasingly abstract concepts.<br><br>

			<em>For example</em>, neural networks can learn how to recognize human faces. How? The first layer of neurons takes pixels from example images, the next layers learn the concept of how pixels form an edge, then that layer passes that knowledge to other
			layers, combining that knowledge of edges to learn the concept of a face. This process of layering knowledge continues until BAM! — the neural network algorithms recognize specific features, and ultimately specific faces.
		</p>

		<h2 id="nlp">Natural Language Processing</h2>

		<img src="https://www.datascience.com/hs-fs/hubfs/Blog/intro-to-natural-language-processing-part-1-lexical-units-plurals.png?t=1487209945146&width=736&height=294&name=intro-to-natural-language-processing-part-1-lexical-units-plurals.png" alt="Natural Language Processing"
		 width="600px">

		<p> AI must communicate with humans as well as humans communicate with each other. In AI, this level of understanding is called Natural Language Understanding, or <a href="https://en.wikipedia.org/wiki/Natural-language_understanding">NLU</a>. NLU is a huge
			priority and challenge in AI research. Why? Because human communication is not straightforward.<br><br> It’s a complex web — random, out-of-order, peppered with humor, emotion and conflict — and it depends hugely on context.Once AI conquers the challenge
			of human communication, decoding complex questions (natural language queries), making connections, and giving answers that make sense, radical progress is not far behind.
		</p>

		<h2 id="Deep-Learning-Breakthroughs">Latest Deep Learning Breakthroughs</h2>

		<h3>&#10687; DeepMind’s AlphaZero Clobbered The Top AI Champions In Go, Shogi, And Chess:</h3>

		<img src="https://blogs-images.forbes.com/mariyayao/files/2018/02/TrainingTime-Graph-171019-r01.gif?" alt="alphazero progress" width="560px">
		<p>Following its stunning win over the best human Go player in 2016, AlphaGo was upgraded a year later into a generalized and more powerful incarnation, AlphaZero. Free of any human guidance except the basic game rules, AlphaZero learned how to play master-level
			chess by itself in just <strong>four hours</strong>. It then proceeded to trounce Stockfish (the top AI chess player) in a 100-game match — without losing a single game.</p>

		<h3>&#10687; These faces show how far AI image generation has advanced in just four years:</h3>

		<img src="https://cdn.vox-cdn.com/thumbor/heTwEWRRGa-ZgiHLkDw5Y4FEjNA=/0x0:920x613/920x613/filters:focal(387x234:533x380):format(webp)/cdn.vox-cdn.com/uploads/chorus_image/image/62694190/ai_face_generation.0.jpg" width="560px" alt="4 years of machine learning advancment">
		<p>In the image above you can see what four years of progress in AI image generation looks like. The crude black-and-white faces on the left are from 2014, published as part of a landmark paper that introduced the AI tool known as the generative adversarial
			network (GAN). The color faces on the right come from a paper published earlier this month, which uses the same basic method but is clearly a world apart in terms of image quality.<br><br> These realistic faces are the work of researchers from Nvidia.
			In their paper, shared publicly last week, they describe modifying the basic GAN architecture to create these images. Take a look at the pictures below. If you didn’t know they were fake, could you tell the difference?</p>

		<img src="https://cdn.vox-cdn.com/thumbor/jkYbdFhNcv1RbS4JJ8rSgG_DXgs=/0x0:1114x556/1120x0/filters:focal(0x0:1114x556):format(webp):no_upscale()/cdn.vox-cdn.com/uploads/chorus_asset/file/13631823/Screen_Shot_2018_12_17_at_3.25.35_PM.png" width="560" alt="Nvidia's AI geberated faces">

		<p>What’s particularly interesting is that these fake faces can also be easily customized. Nvidia’s engineers incorporated a method known as style transfer into their work, in which the characteristics of one image are blended with another. You might recognize
			the term from various image filters that are popular on apps like Prisma and Facebook in recent years, which can make your selfies look like an impressionist painting or a cubist work of art.<br><br> Here's a video about an <em>Application</em> called
			<strong>Deep Fakes</strong> that demostrates furthermore the capabilities of these methods:
		</p>

		<iframe width="560" height="315" src="https://www.youtube.com/embed/C8FO0P2a3dA" style="border:0;" allowfullscreen></iframe>

		<h3>&#10687; Quantum & Optical Computing Entered The AI Hardware Wars:</h3>

		<img src="https://www.21stcentech.com/wp-content/uploads/2015/05/quantum_computing.png" width="560px" alt="Quantum computer">
		<p>Faster hardware means better AI. Google announced a 2nd generation of their Tensor Processing Units (TPU) designed specifically for AI research, but technically innovative competitors like quantum computing and optical computing have entered the fray.
			IBM and Google both announced respective milestones in their quantum computing progress, while researchers and engineers discovered that matrix operations used in deep learning can be done in parallel by switching from an electrical computing paradigm
			to a phototonic one.<br><br> Here's a video that explains more about Quantum Computing:
		</p>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/JhHMJCUmq28" style="border:0;" allowfullscreen></iframe>

		<p style="margin-top:50px;margin-bottom:-20px;">Artificial Intelligence is going to be crucial in our daily lifes for the next years.....<br>Be ready for it!</p>
		
	</div>
	<div id="footer"> Website by <a class="effect-shine" href="https://www.linkedin.com/in/omar-qaterge">Omar Qaterge</a>.<br/> Made with &#x2764;&#xFE0F; for Hack Your Future</div>
</body>
